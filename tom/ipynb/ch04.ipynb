{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.Javascript object>",
      "application/javascript": "if(MathJax) {\n    MathJax.Hub.Config({\n        TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n    });\n    MathJax.Hub.Queue(\n    [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n    [\"PreProcess\", MathJax.Hub],\n    [\"Reprocess\", MathJax.Hub]\n  );\n}\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%javascript\n",
    "if(MathJax) {\n",
    "    MathJax.Hub.Config({\n",
    "        TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
    "    });\n",
    "    MathJax.Hub.Queue(\n",
    "    [\"resetEquationNumbers\", MathJax.InputJax.TeX],\n",
    "    [\"PreProcess\", MathJax.Hub],\n",
    "    [\"Reprocess\", MathJax.Hub]\n",
    "  );\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの学習\n",
    "\n",
    "[データ] -> (人の考えたアルゴリズム) -> [答え]\n",
    "[データ] -> (人の考えた特徴量) -> (機械学習 SVM, KNNなど） -> [答え]\n",
    "[データ] -> (ニューラルネットワーク（ディープラーニング) -> [答え]\n",
    "\n",
    "ニューラルネットワーク（ディープラーニング）は，特徴量の抽出という人間の介在がなくとも，入力からそのまま学習することができる．\n",
    "\n",
    "### 損失関数 loss function or cost function\n",
    "\n",
    "1. Mean Squared Error 二乗和誤差\n",
    "\n",
    "NNの出力と教師ラベルの誤差を二乗して合計したもの\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{2} \\sum_k(y_k-t_k)^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "2. Cross Entropy Error 交差エントロピー誤差\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E = -\\sum_k t_k log y_k\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$t_k$は，正解ラベルとなるインデックスだけが1で，その他は0であるというone-hot表現であるとする．\n",
    "とすると，この式はつまり，正解した出力のlogを合計したものである．\n",
    "$log 1 = 0$であることから，すべての正解ラベルに対して，$y_k=1$を出力できた場合，つまり全問正解のとき，$E=0$となる．\n",
    "これに対して，正解ラベルに対して，$y_k<1$を出力したものが誤差として加算されていく．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t) ** 2)\n",
    "\n",
    "# deltaを足すのは，log(0) = -infになるのを避けるため\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ミニバッチ学習\n",
    "\n",
    "訓練データすべてから損失関数の和をもとめるには，訓練データのサンプル数をNとして，交差エントロピー誤差の場合，上記式を拡張して，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E = - \\frac{1}{N} \\sum_n \\sum_k t_{nk} log y_{nk}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "となる．ｍnistの場合，訓練データが約60000あり，個々のデータにはk個の出力があるわけなので，なかなか時間がかかってしまう．\n",
    "そこで，訓練データの中からサンプリングすることで近似値を求めながら学習させるのが，ミニバッチ学習である．\n",
    "\n",
    "## エポック\n",
    "\n",
    "エポックとは，訓練データを一通りすべて使いきった回数．10000個の訓練データをミニバッチサイズ1000で訓練させていたら，ミニバッチ10回で，1エポックとなる．\n",
    "\n",
    "## なぜ誤差関数を設定するのか？\n",
    "\n",
    "mnistの場合は，正解かどうかの「認識精度」をそのまま指標とすればよいのではないだろうか？\n",
    "A. 学習時に最適なパラメータを探索する際には，微分が用いられるが，認識精度を指標にしてしまうと，ほとんどのところで０になり，微分ができず，うまく学習（パラメータ更新）ができないからである．\n",
    "正答率を指標にした場合，その指標の変化は非連続である（離散である）．しかし，損失関数を用いると，あたかも精度が連続値のように扱うことができ，微分可能になるのである．\n",
    "\n",
    "## 勾配の求め方\n",
    "\n",
    "通常学習するパラメータは複数あるため，それぞれのパラメータの偏微分を求めてから，それらをベクトルとしてまとめたものを，勾配という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[6. 8.]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 微分に用いるdelta\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        # 各パラメータごとに偏微分を求める\n",
    "        tmp = x[idx]\n",
    "        \n",
    "        x[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp\n",
    "    return grad\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 勾配法\n",
    "\n",
    "最適なパラメータとは，損失関数が最小となる場合であり，勾配の示す方向が，その損失を最小化する方向である．\n",
    "\n",
    "勾配法は，この勾配の進む方向にある一定距離勧め，移動した先でまた勾配を評価して進む，ということを繰り返して，最小値にたどり着く手法である．\n",
    "\n",
    "Gradient descent method = 勾配降下法\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "x_0 = x_0 - \\eta \\frac{\\partial f}{\\partial x_0} \\\\\n",
    "x_1 = x_1 - \\eta \\frac{\\partial f}{\\partial x_1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\eta$ は，learning rate = 学習率．ハイパーパラメータとして事前に設定する必要がある．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配法の実装\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([6.11110793e-10, 8.14814391e-10])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# x_0^2 + x_1^2 の最小値を求める\n",
    "def func(x):\n",
    "    return np.sum(x**2)\n",
    "\n",
    "init_x = np.array([3.0, 4.0])\n",
    "gradient_descent(func, init_x, lr=0.1) # 結果，だいたい(0, 0)に近い値が求められる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの勾配\n",
    "\n",
    "ある層において，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "W = \\begin{pmatrix}\n",
    "  w_{11} & w_{12} & w_{13} \\\\\n",
    "  w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "というWeightが与えられ，その損失が，$L$であるとすると，\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{ \\partial L }{ \\partial W } = \\begin{pmatrix}\n",
    "  \\frac{ \\partial L }{ \\partial w_{11}} & \\frac{ \\partial L }{ \\partial w_{12} } & \\frac{ \\partial L }{ \\partial w_{13} } \\\\\n",
    "  \\frac{ \\partial L }{ \\partial w_{21}} & \\frac{ \\partial L }{ \\partial w_{22} } & \\frac{ \\partial L }{ \\partial w_{23} }\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "が，その損失の勾配となる．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[-0.85962715  0.26073598 -0.36334443]\n [-1.07225197 -1.06688374 -1.02781945]]\n"
    }
   ],
   "source": [
    "import sys, os\n",
    "import os.path as path\n",
    "sys.path.append(path.abspath(path.join(os.curdir ,\"../..\")))\n",
    "\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "## Simpleなニューラルネットの実装\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        return cross_entropy_error(y, t)\n",
    "\n",
    "net = SimpleNet()\n",
    "print(net.W)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1.48074306 -0.80375378 -1.14304417]\n0.7976962138947823\n"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 1, 0])\n",
    "print(net.predict(x))\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 0.137304   -0.32977709  0.19247309]\n [ 0.205956   -0.49466563  0.28870963]]\n"
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "# 勾配を求める\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットの学習の実装\n",
    "\n",
    "0. (前提)ニューラルネットには，「重み」と「バイアス」というパラメータがある\n",
    "1. 損失関数\n",
    "2. ミニバッチ\n",
    "3. 勾配\n",
    "4. 勾配降下法（ミニバッチを使うため，確率的勾配降下法=stochastic gradient descent sgdという）\n",
    "\n",
    "を組み合わせることで，実装可能．まずは，2層のネットワークからやってみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(784, 50)\n(50,)\n(50, 10)\n(10,)\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "load_mnist() got an unexpected keyword argument 'one_host_label'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3d757199c2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m## Train in mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_host_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m## Hyper params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_mnist() got an unexpected keyword argument 'one_host_label'"
     ]
    }
   ],
   "source": [
    "from common.functions import sigmoid, softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class Tom2Net:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        a1 = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "        return softmax(a2)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        accu = np.sum(y == t) / float(x.shape[0])\n",
    "        return accu\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        pass\n",
    "\n",
    "## Sample usage\n",
    "n = Tom2Net(28**2, 50, 10)\n",
    "x = np.random.rand(100, 28**2) ## dummy data x 100\n",
    "t = np.random.rand(100, 10) ## dummy label * 100\n",
    "y = n.predict(x)\n",
    "grads = n.numerical_gradient(x, t)\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)\n",
    "\n",
    "## Train in mini-batch\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "## Hyper params\n",
    "iter_num = 10_000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # ミニバッチの数\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "net = Tom2Net(28**2, 50, 10)\n",
    "\n",
    "for i in range(iter_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "\n",
    "    grad = net.numerical_gradient(x_batch, y_batch)\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        n.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # Training 履歴の記録\n",
    "    loss = net.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1epochごとにaccuracyを更新\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = net.accuracy(x_train, y_train)\n",
    "        test_acc = net.accuracy(x_test, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"train acc, test acc | {str(train_acc)}, {str(test_acc)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし，この`numerical_gradient`はあまりに遅すぎる．．というわけで，次章では，勾配を求める偏微分を高速化する「誤差逆伝播法」（Back propagation）について学ぶ"
   ]
  }
 ]
}